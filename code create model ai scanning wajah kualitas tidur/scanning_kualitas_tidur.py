# -*- coding: utf-8 -*-
"""Scanning Kualitas Tidur.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12fhUgd3-BNffjGhs-4BfNjmjofn4sc6U

# üí§ Scanning Kualitas Tidur

## 0¬†¬†Mount Google‚ÄØDrive & cek model
"""

from pathlib import Path
from google.colab import drive
from tensorflow.keras.models import load_model

drive.mount('/content/drive')

DRIVE_DIR = Path('/content/drive/MyDrive/sleep_quality_cnn')
DRIVE_DIR.mkdir(exist_ok=True)
MODEL_PATH = DRIVE_DIR / 'latest.h5'

if MODEL_PATH.exists():
    print(f'‚úÖ Model ditemukan ‚áí {MODEL_PATH}')
    model = load_model(MODEL_PATH)
    SKIP_TRAINING = True
else:
    print('‚ùå Model tidak ditemukan ‚Äî¬†akan lanjut download dataset & training.')
    SKIP_TRAINING = False

"""## 1¬†¬†Download dataset FER‚Äë2013 (hanya jika training)"""

if not SKIP_TRAINING:
    # --- Upload / pastikan kaggle.json ---
    from google.colab import files, runtime
    import os, shutil, subprocess, pathlib

    KAGGLE_PATH = Path.home()/'.kaggle'
    KAGGLE_PATH.mkdir(exist_ok=True)

    if not (KAGGLE_PATH/'kaggle.json').exists():
        print('üîë Upload kaggle.json untuk autentikasi Kaggle API')
        files.upload()  # user selects kaggle.json
        shutil.move('kaggle.json', KAGGLE_PATH/'kaggle.json')
        os.chmod(KAGGLE_PATH/'kaggle.json', 0o600)
    else:
        print('‚úÖ kaggle.json sudah ada.')

    # --- Download dataset ---
    # !kaggle datasets download -d msambare/fer2013 -p data --unzip --force
    # --- Download dataset ---
    !kaggle datasets download -d genadieva/fer-2013-csv-dataset -p data --unzip --force

"""## 2¬†¬†Pre‚Äëprocessing dataset"""

if not SKIP_TRAINING:
    import pandas as pd, numpy as np
    from sklearn.model_selection import train_test_split

    df = pd.read_csv('data/fer2013.csv')

    # Convert pixel string to 48x48 grayscale image
    X = np.array([np.fromstring(pix, sep=' ', dtype='float32').reshape(48,48,1) for pix in df['pixels']]) / 255.0
    y = pd.get_dummies(df['emotion']).values

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    print('Dataset ready:', X_train.shape, y_train.shape)

"""## 3¬†¬†Bangun & training CNN"""

if not SKIP_TRAINING:
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

    model = Sequential([
        Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),
        BatchNormalization(), MaxPooling2D(2,2),
        Conv2D(64, (3,3), activation='relu'),
        BatchNormalization(), MaxPooling2D(2,2),
        Conv2D(128, (3,3), activation='relu'),
        BatchNormalization(), MaxPooling2D(2,2),
        Flatten(),
        Dense(256, activation='relu'), Dropout(0.5),
        Dense(7, activation='softmax')
    ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    EPOCHS = 15
    model.fit(X_train, y_train, epochs=EPOCHS, batch_size=64, validation_split=0.1)

"""## 4¬†¬†Simpan model ke Google‚ÄØDrive"""

if not SKIP_TRAINING:
    from datetime import datetime

    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    saved_path = DRIVE_DIR / f'model_{timestamp}.h5'
    model.save(saved_path)
    print('üíæ Disimpan:', saved_path)

    # update latest.h5
    latest = DRIVE_DIR / 'latest.h5'
    latest.write_bytes(saved_path.read_bytes())
    print('üîÅ Diperbarui latest.h5')

# =========================================================
# 0) Hentikan server lama (jika ada)
# =========================================================
!fuser -k 9000/tcp || true

# =========================================================
# 1) Install library (tanpa tensorflow)
# =========================================================
!pip -q install fastapi uvicorn nest_asyncio pyngrok pillow

# =========================================================
# 2) Import Library
# =========================================================
import nest_asyncio, threading, io, numpy as np, os, time
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from tensorflow.keras.models import load_model   # TF bawaan Colab
from PIL import Image
from pyngrok import ngrok
import uvicorn

nest_asyncio.apply()

# =========================================================
# 3) Token Ngrok
# =========================================================
ngrok.set_auth_token("AUTH TOKEN NGROK")

# =========================================================
# 4) Mount Google Drive & Load Model
# =========================================================
from google.colab import drive
drive.mount('/content/drive')

MODEL_PATH = "/content/drive/MyDrive/sleep_quality_cnn/latest.h5"
assert os.path.exists(MODEL_PATH), f"Model tidak ditemukan di {MODEL_PATH}"
print("‚è≥  Loading model...")
model = load_model(MODEL_PATH)
print("‚úÖ  Model loaded.")

LABELS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']

# =========================================================
# 5) FastAPI App
# =========================================================
app = FastAPI(title="Sleep-Quality Face Detector")

@app.post("/predict")
async def predict(image: UploadFile = File(...)):
    try:
        img_bytes = await image.read()
        img = Image.open(io.BytesIO(img_bytes)).convert('L').resize((48, 48))
        arr = np.array(img).reshape(1, 48, 48, 1) / 255.0
        pred = model.predict(arr)[0]
        idx = int(np.argmax(pred))
        return {"emotion": LABELS[idx], "confidence": float(pred[idx])}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# =========================================================
# 6) Jalankan Uvicorn di Background (Port 9000)
# =========================================================
def run_api():
    uvicorn.run(app, host="0.0.0.0", port=9000, log_level="info")

threading.Thread(target=run_api, daemon=True).start()
time.sleep(3)  # Tunggu server siap

# =========================================================
# 7) Ngrok Tunnel
# =========================================================
public_url = ngrok.connect(9000)
print("\nüöÄ API sudah siap!")
print(f"POST endpoint: {public_url}/predict")
print(f"Swagger UI: {public_url}/docs")

# =========================================================
# 0) Hentikan proses lama di port 9000 (jika ada)
# =========================================================
!fuser -k 9000/tcp || true

# =========================================================
# 1) Install library (sekali per session)
# =========================================================
!pip -q install fastapi uvicorn nest_asyncio pyngrok tensorflow pillow

# =========================================================
# 2) Import & patch event loop (Colab butuh)
# =========================================================
import nest_asyncio, threading, io, numpy as np, os, time
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from tensorflow.keras.models import load_model
from PIL import Image
from pyngrok import ngrok
import uvicorn

nest_asyncio.apply()

# =========================================================
# 3) Pasang token Ngrok (ganti jika berbeda)
# =========================================================
ngrok.set_auth_token("AUTH TOKEN NGROK")

# =========================================================
# 4) Mount Google Drive & load model
# =========================================================
from google.colab import drive
drive.mount('/content/drive')

MODEL_PATH = "/content/drive/MyDrive/sleep_quality_cnn/latest.h5"
assert os.path.exists(MODEL_PATH), f"Model tidak ditemukan di {MODEL_PATH}"
print("‚úÖ  Loading model:", MODEL_PATH)
model = load_model(MODEL_PATH)

LABELS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']

# =========================================================
# 5) Definisikan FastAPI
# =========================================================
app = FastAPI(title="Sleep‚ÄëQuality Face Detector")

@app.post("/predict")
async def predict(image: UploadFile = File(...)):
    try:
        img_bytes = await image.read()
        img = Image.open(io.BytesIO(img_bytes)).convert('L').resize((48, 48))
        img_arr = np.array(img).reshape(1, 48, 48, 1) / 255.0
        pred = model.predict(img_arr)[0]
        idx = int(np.argmax(pred))
        return {"emotion": LABELS[idx], "confidence": float(pred[idx])}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# =========================================================
# 6) Jalankan Uvicorn di thread terpisah (port 9000)
# =========================================================
def run_api():
    uvicorn.run(app, host="0.0.0.0", port=9000, log_level="info")

thread = threading.Thread(target=run_api, daemon=True)
thread.start()

# Tunggu server siap
time.sleep(2)

# =========================================================
# 7) Buat tunnel Ngrok & tampilkan URL publik
# =========================================================
public_url = ngrok.connect(9000)
print("\nüéØ  URL publik:")
print(f"{public_url}/predict")
print(f"{public_url}/docs  (Swagger UI)")

"""---
### üìà Jika `SKIP_TRAINING = True`
Kamu bisa langsung melakukan evaluasi atau menyimpan ulang model, atau cukup tutup notebook.

Selamat mencoba dan semoga sukses!
"""